{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import add_messages, START, END, StateGraph\n",
    "from langgraph.types import Command, Interrupt, interrupt\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    linkedIn_topic: str\n",
    "    generated_post: Annotated[List[str], add_messages]\n",
    "    human_feedback: Annotated[List[str], add_messages]\n",
    "\n",
    "def model(state: State):\n",
    "    \"Here we are using the LLM to generate a linkedIn post with human feedback incorparated\"\n",
    "    print(\"[model]-Generating agent\")\n",
    "    linkedin_topic = state[\"linkedIn_topic\"]\n",
    "    # feedback= state.get(\"human_feedback\", \"no feedback yet\")\n",
    "    feedback = state[\"human_feedback\"] if \"human_feedback\" in state else [\"No Feedback yet\"]\n",
    "    prompt = f\"\"\"\n",
    "    LinkedIn Topic: {linkedin_topic}\n",
    "    Human Feedback: {feedback[-1] if feedback else \"No feedback\"}\n",
    "    Generate a structured and well-written linkedIn Post on the given topic.\n",
    "    Consider previous human feedback to refine the reponse.\n",
    "    \"\"\"\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are an expert LinkedIn content writer\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ])\n",
    "    generated_ln_post=response.content\n",
    "    print(f\"[model_node] Generated post:\\n {generated_ln_post} \\n\")\n",
    "    return{\n",
    "        \"generated_post\":[AIMessage(content=generated_ln_post)],\n",
    "        \"human_feedback\":feedback\n",
    "    }\n",
    "def human_node(state:State):\n",
    "    \"\"\"Human Intervention node - loops back to model unless input is done\"\"\"\n",
    "    print(\"\\n [human_node] awaiting human feedback...\")\n",
    "    generated_post = state[\"generated_post\"]\n",
    "    user_feedback = interrupt(\n",
    "        {\n",
    "            \"generated_post\": generated_post, \n",
    "            \"message\": \"Provide feedback or type 'done' to finish\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if user_feedback.lower() == \"done\":\n",
    "        return Command(update={\"human_feedback\":state[\"human_feedback\"] + [\"finalised\"]}, goto=\"end_node\")\n",
    "    return Command(update={\"human_feedback\": state[\"human_feedback\"] + [user_feedback]}, goto=\"model\")\n",
    "def end_node(state:State):\n",
    "    \"\"\"Final Code\"\"\"\n",
    "    print(\"\\n[end_node] Process finished\")\n",
    "    print(\"Final Generated Post:\", state[\"generated_post\"][-1])\n",
    "    return{\"generated_post\": state[\"generated_post\"], \"human_feedback\":state[\"human_feedback\"]}\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"model\", model)\n",
    "graph.add_node(\"human_node\", human_node)\n",
    "graph.add_node(\"end_node\", end_node)\n",
    "graph.set_entry_point(\"model\")\n",
    "\n",
    "graph.add_edge(START, \"model\")\n",
    "graph.add_edge(\"model\", \"human_node\")\n",
    "graph.set_finish_point(\"end_node\")\n",
    "\n",
    "# Enable Interupt\n",
    "checkpointer = MemorySaver()\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "thread_config = {\"configurable\": {\n",
    "    \"thread_id\": uuid.uuid4()\n",
    "}}\n",
    "\n",
    "linkedIn_topic = input(\"Enter your linkedIn topic: \")\n",
    "initial_state:State ={\n",
    "    \"linkedIn_topic\": linkedIn_topic,\n",
    "    \"generated_post\": [],\n",
    "    \"human_feedback\": []\n",
    "}\n",
    "\n",
    "for chunk in app.stream(initial_state, config=thread_config):\n",
    "    print(chunk,\"chunk\")\n",
    "    for node_id, value in chunk.items():\n",
    "        if node_id == '__interupt__':\n",
    "            while True:\n",
    "                 user_feedback = input(\"Provide feedback (or type 'done' when finished): \")\n",
    "                 app.invoke(Command(resume=user_feedback), config=thread_config)\n",
    "\n",
    "                 if user_feedback.lower() == \"done\":\n",
    "                    break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
