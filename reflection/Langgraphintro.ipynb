{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key features of langGraph\n",
    "\n",
    "1. Looping and branching capabilities\n",
    "\n",
    "Supports conditional statements and loop structures, allowing dynamic excution based on route\n",
    "\n",
    "2. State Persistance:\n",
    "Automatically preserves state supporting pause and resume for long running conversations\n",
    "\n",
    "3. Human Machine interaction support\n",
    "\n",
    "Allows inserting of human review during execution, supporting state editing and modification with flexible interaction control mechanism\n",
    "\n",
    "4. Streaming processing\n",
    "supports streaming output and real-time feedback on execution status to enhance user experience\n",
    "\n",
    "5. Seemless Integration with langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Components of LangGraph\n",
    "\n",
    "1. Nodes\n",
    "2. edges\n",
    "3. conditional edges\n",
    "4. state\n",
    "\n",
    "# Reflection Agent Pattern\n",
    "\n",
    "N.B will always have something that generates something and another that critics this \n",
    "\n",
    "A reflection agent pattern is an AI system pattern that can look at its own outputs and think about them/ make them better\n",
    "A basic reflection agent system typically consist of:\n",
    "1. A generator agent\n",
    "2. A reflector agent\n",
    "\n",
    "# Types of Reflection Agents\n",
    "1. Basic Reflection agents\n",
    "2. Reflexion agents\n",
    "3. Language Agents Tree Search(LATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Reflection agent\n",
    "\n",
    "# CHAINS \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "generation_prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"You are a tweeter techie tasked with generating a tweet about the user's input. \"\n",
    "            \"Generate the best tweet possible. \"\n",
    "            \"If the user criticizes your tweet, reflect on it and improve it.\"),\n",
    "MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "\n",
    "reflect_prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"You are a viral Twitter influencer grading a tweet. \"\n",
    "            \"Generate a critique and recommendation for the user's tweet. \"\n",
    "            \"Always provide a critique and recommendation. \"\n",
    "            \"If the user criticizes your critique, reflect on it and improve it.\"),\n",
    "MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-1.5-pro\",\n",
    "#     temperature=0.2,\n",
    "#     max_output_tokens=100,\n",
    "# )\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "generation_chain = generation_prompt | llm\n",
    "reflect_chain = reflect_prompt | llm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MessageGraph: is a class provided by langGraph that we can use to orchestrate the flow of messages between different nodes. \n",
    "use cases include simplechatbox, routing decisions etc\n",
    "If your app requires complex state manages consider using the `stateGraph`\n",
    "Each node recieves the entire list of messages and can append to it or modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables...\n",
      "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\n",
      "Checking if tracing is enabled...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "utils.get_env_var.cache_clear()\n",
    "\n",
    "print(\"Loading environment variables...\")\n",
    "load_dotenv()\n",
    "print(os.environ.get(\"LANGSMITH_API_KEY\"))\n",
    "print(\"Checking if tracing is enabled...\")\n",
    "# print(os.environ)\n",
    "\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__(<p>__start__</p>)\n",
      "\tgenerate(generate)\n",
      "\treflect(reflect)\n",
      "\t__end__(<p>__end__</p>)\n",
      "\t__start__ --> generate;\n",
      "\tgenerate --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "+----------+   \n",
      "| generate |   \n",
      "+----------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | __end__ |   \n",
      " +---------+   \n",
      "Response: [HumanMessage(content=\"I love the new features in the latest iPhone update! Can't wait to try them out.\", additional_kwargs={}, response_metadata={}, id='d730f646-f8ae-4081-b645-fb5ca13da243'), AIMessage(content=\"Unboxing the future! üì±‚ú® Just updated my iPhone and the new features are blowing my mind. Can't wait to dive in and explore all the possibilities! Who else is excited? #iPhoneUpdate #TechLovers #AppleMagic üçèüì≤\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 68, 'total_tokens': 121, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BimLI0zdoCkaBCpIxw1PGUSYbEcK3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--43d80cca-d235-4fc4-8910-2926a0951c83-0', usage_metadata={'input_tokens': 68, 'output_tokens': 53, 'total_tokens': 121, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Your tweet is positive and personal, which is great for connecting with your audience. However, to make it more engaging and spark conversations, consider adding specifics about which features excite you the most. This will encourage others to share their thoughts and create a dialogue.\\n\\nRecommendation: Highlight a particular new feature you‚Äôre excited about and use hashtags to increase your tweet\\'s visibility. For instance:\\n\\n\"Thrilled about the new Siri shortcuts in the latest iPhone update! Can\\'t wait to automate my morning routine with just a voice command. What‚Äôs your favorite feature? #iPhoneUpdate #TechTalk üó£Ô∏èüì±\"\\n\\nThis approach invites interaction and offers followers a chance to engage more deeply with your content.', additional_kwargs={}, response_metadata={}, id='80d4c478-3678-42f7-9c7b-be5e8cc4d657'), AIMessage(content='Great advice! Let me give it another shot:\\n\\n\"Excited to dive into the new iPhone update‚Äîespecially pumped about the StandBy mode for charging! üì±‚ö° It\\'s going to change my nightstand game. What feature are you vibing with the most? #iPhoneUpdate #TechTalk üåôüí°\"\\n\\nThanks for the feedback!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 269, 'total_tokens': 341, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BimLPsaQ94liY2H8uZpqu7T9ERojK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d0718e72-89cd-46c6-8c22-b4b91cae03c3-0', usage_metadata={'input_tokens': 269, 'output_tokens': 72, 'total_tokens': 341, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Sequence\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "load_dotenv()\n",
    "\n",
    "REFLECT = \"reflect\"\n",
    "GENERATE = \"generate\"\n",
    "graph = MessageGraph()\n",
    "\n",
    "\n",
    "def generate_node(state):\n",
    "    return generation_chain.invoke({\n",
    "        \"messages\": state}\n",
    "    )\n",
    "\n",
    "def reflect_node(state):\n",
    "    response =reflect_chain.invoke({\n",
    "        \"messages\": state}\n",
    "    )\n",
    "    return[ HumanMessage(\n",
    "        content=response.content\n",
    "    )]\n",
    "\n",
    "graph.add_node(\n",
    "    GENERATE,\n",
    "    generate_node,\n",
    "    # input_variables=[\"messages\"],\n",
    "    # output_variables=[\"messages\"],\n",
    ")\n",
    "graph.add_node(\n",
    "    REFLECT,\n",
    "    reflect_node,\n",
    "    # input_variables=[\"messages\"],\n",
    "    # output_variables=[\"messages\"],\n",
    ")\n",
    "\n",
    "graph.set_entry_point(GENERATE)\n",
    "\n",
    "def should_continue(state: Sequence[BaseMessage]):\n",
    "    \"\"\"\n",
    "    Check if the last message is a critique.\n",
    "    \"\"\"\n",
    "    # if not state:\n",
    "    #     return True\n",
    "    # last_message = state[-1]\n",
    "    # return isinstance(last_message, AIMessage) and \"critique\" not in last_message.content.lower()\n",
    "    if len(state) > 2:\n",
    "        return END\n",
    "    return REFLECT\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    GENERATE,\n",
    "    should_continue,\n",
    "    # {\n",
    "    #     True: REFLECT,\n",
    "    #     False: END,\n",
    "    # },\n",
    ")\n",
    "\n",
    "graph.add_edge(REFLECT, GENERATE)\n",
    "app = graph.compile()\n",
    "print(app.get_graph().draw_mermaid())\n",
    "app.get_graph().print_ascii()\n",
    "\n",
    "response =app.invoke(HumanMessage(\n",
    "    content=\"I love the new features in the latest iPhone update! Can't wait to try them out.\",\n",
    "))\n",
    "\n",
    "print(\"Response:\", response)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
